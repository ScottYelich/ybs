{
  "llm": {
    "provider": "ollama",
    "model": "qwen2.5:1.5b",
    "endpoint": "http://localhost:11434/v1/chat/completions",
    "api_key": "not-needed-for-ollama",
    "temperature": 0.7,
    "max_tokens": 2048
  }
}
